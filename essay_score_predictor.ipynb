{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import codecs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "#Used to generate the same set of random numbers when random function is called\n",
    "np.random.seed(7)\n",
    "\n",
    "#Read The TSV Data File\n",
    "data = pd.read_table('/home/monark/LEARNINGS/Projects/Automated_Grading_System/train.tsv')\n",
    "data1 = pd.read_table('train_rel_2.tsv')\n",
    "\n",
    "#Because of small size of the dataset O merged both the files given for training\n",
    "data = [data, data1]\n",
    "data = pd.concat(data)\n",
    "\n",
    "\n",
    "essay_text = data['EssayText']\n",
    "essay_score = data['Score1']\n",
    "essay_set = data['EssaySet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce 820M (CNMeM is enabled with initial size: 85.0% of memory, cuDNN not available)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34250 texts.\n",
      "Found 16729 unique tokens.\n",
      "Shape of data tensor: (34250, 500)\n",
      "Shape of label tensor: (34250, 4)\n"
     ]
    }
   ],
   "source": [
    "## \n",
    "# RAW DATA ENCODING\n",
    "# Format the text samples and labels into tensors that can be fed into a neural network\n",
    "##\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "sets = [0,1,2,3] # The 4 different classes of Scores\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "\n",
    "for label in sets:\n",
    "\tlabel_id = len(labels_index)\n",
    "\tlabels_index[label] = label_id\n",
    "\tfor t in essay_text[data['Score1']==label]:\n",
    "\t\ttexts.append(t)\n",
    "\t\tlabels.append(label_id)\n",
    "\n",
    "\n",
    "print('Found %s texts.' % len(texts))\n",
    "\n",
    "top_words = 5000 #top most-frequent words extracted from the dataset\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=top_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "max_response_length = 500\n",
    "data = pad_sequences(sequences, maxlen=max_response_length)\n",
    "\n",
    "#Convert class vector to binary class matrix, for use with categorical_crossentropy.\n",
    "#Or in simple words to convert numbers into ONE-HOT Vector for MultiClass classification\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "\n",
    "#Shuffle the dataset \n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Set of stopwords from nltk.corpus package\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "word_counts_X = []  # List of word Counts in each example response\n",
    "for x in essay_text:\n",
    "    word_counts_X.append(len([i for i in word_tokenize(x) if i not in stop]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_counts = []  # Set of sentence counts in each example response\n",
    "for x in essay_text:\n",
    "    sent_counts.append(len(sent_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Shuffle all of the data\n",
    "word_counts_X = np.array(word_counts_X)\n",
    "word_counts_X = word_counts_X[indices]\n",
    "\n",
    "sent_counts = np.array(sent_counts)\n",
    "sent_counts = sent_counts[indices]\n",
    "\n",
    "essay_set = np.array(essay_set)\n",
    "essay_set = essay_set[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 3344,\n",
       "         2: 2556,\n",
       "         3: 3699,\n",
       "         4: 3395,\n",
       "         5: 3590,\n",
       "         6: 3594,\n",
       "         7: 3598,\n",
       "         8: 3598,\n",
       "         9: 3596,\n",
       "         10: 3280})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To check whether the dataset is balanced or not\n",
    "from collections import Counter\n",
    "Counter(essay_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29112, 500)\n",
      "(29112, 3)\n"
     ]
    }
   ],
   "source": [
    "#Size for train-test split\n",
    "train_size = int(0.85 * len(data))\n",
    "\n",
    "X_train = data[:train_size]\n",
    "X_test = data[train_size:]\n",
    "\n",
    "set_train = essay_set[:train_size]\n",
    "set_test = essay_set[train_size:]\n",
    "\n",
    "sent_count_train = sent_counts[:train_size]\n",
    "sent_count_test = sent_counts[train_size:]\n",
    "\n",
    "word_count_test = word_counts_X[train_size:]\n",
    "word_count_train = word_counts_X[:train_size]\n",
    "\n",
    "#Concatenate the three data-vectors into a single matrix or feature-set\n",
    "features_train = np.column_stack((set_train,sent_count_train,word_count_train))\n",
    "features_test = np.column_stack((set_test,sent_count_test,word_count_test))\n",
    "\n",
    "y_train = labels[:train_size]\n",
    "y_test = labels[train_size:]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(features_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "text (InputLayer)                (None, 500)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)          (None, 500, 32)       160000      text[0][0]                       \n",
      "____________________________________________________________________________________________________\n",
      "lstm_13 (LSTM)                   (None, 500, 100)      53200       embedding_7[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lstm_14 (LSTM)                   (None, 150)           150600      lstm_13[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "features (InputLayer)            (None, 3)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "merge_7 (Merge)                  (None, 153)           0           lstm_14[0][0]                    \n",
      "                                                                   features[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 153)           0           merge_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 200)           30800       dropout_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "elu_7 (ELU)                      (None, 200)           0           dense_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "score (Dense)                    (None, 4)             804         elu_7[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 395404\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Model Design\n",
    "#Functional Model\n",
    "\n",
    "from keras.layers import Dense, Dropout, Activation, Input\n",
    "from keras.layers import LSTM,merge\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.regularizers import l2\n",
    "\n",
    "#Embedding-Output Vector Length\n",
    "embedding_vector_length = 32\n",
    "\n",
    "#Define Text-Input\n",
    "text_in = Input(shape=(500,), name='text')\n",
    "\n",
    "#Embeddings\n",
    "#Used to convert the encoded data i.e the matrix of indices into a form compatible with LSTM\n",
    "embedding = Embedding(output_dim=embedding_vector_length, input_dim=top_words, input_length=500)(text_in)\n",
    "\n",
    "#LSTM\n",
    "lstm_1= LSTM(100, return_sequences = True)(embedding)\n",
    "lstm_2 = LSTM(150)(lstm_1)\n",
    "\n",
    "#Features Inputs (essay_set, word_counts, sent_counts)\n",
    "features_in = Input(shape=(3,), name='features')\n",
    "\n",
    "#Merge layer to merge the output of LSTM and the feature inputs\n",
    "x = merge([lstm_2, features_in], mode='concat')\n",
    "\n",
    "#Dropout for the hidden_units to be independent from each other\n",
    "dropout = Dropout(0.2)(x)\n",
    "\n",
    "#Hidden Dense Layer\n",
    "D1 = Dense(200,W_regularizer=l2(0.01),b_regularizer = l2(0.01))(dropout) # try 150 dense after this\n",
    "ED1 = ELU()(D1)\n",
    "\n",
    "# Final Dense Output-Layer\n",
    "score = Dense(4, activation='softmax', name='score',W_regularizer=l2(0.01),b_regularizer = l2(0.01))(ED1) \n",
    "\n",
    "#model\n",
    "model = Model(input=[text_in, features_in], output=[score])\n",
    "\n",
    "#optimizer\n",
    "adam = Adam(lr = 0.001)#, decay = 1e-4) \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26200 samples, validate on 2912 samples\n",
      "Epoch 1/20\n",
      "26200/26200 [==============================] - 663s - loss: 1.1203 - acc: 0.6412 - val_loss: 0.6958 - val_acc: 0.6954\n",
      "Epoch 2/20\n",
      "26200/26200 [==============================] - 662s - loss: 0.7272 - acc: 0.7366 - val_loss: 0.5990 - val_acc: 0.7521\n",
      "Epoch 3/20\n",
      "26200/26200 [==============================] - 662s - loss: 0.6306 - acc: 0.7829 - val_loss: 0.5512 - val_acc: 0.7785\n",
      "Epoch 4/20\n",
      "26200/26200 [==============================] - 663s - loss: 0.5670 - acc: 0.8151 - val_loss: 0.5434 - val_acc: 0.7874\n",
      "Epoch 5/20\n",
      "26200/26200 [==============================] - 663s - loss: 0.5175 - acc: 0.8406 - val_loss: 0.5189 - val_acc: 0.8070\n",
      "Epoch 6/20\n",
      "26200/26200 [==============================] - 663s - loss: 0.4673 - acc: 0.8636 - val_loss: 0.4985 - val_acc: 0.8091\n",
      "Epoch 7/20\n",
      "26200/26200 [==============================] - 663s - loss: 0.4262 - acc: 0.8818 - val_loss: 0.4859 - val_acc: 0.8187\n",
      "Epoch 8/20\n",
      "26200/26200 [==============================] - 663s - loss: 0.3905 - acc: 0.8941 - val_loss: 0.5588 - val_acc: 0.8115\n",
      "Epoch 9/20\n",
      "26200/26200 [==============================] - 663s - loss: 0.3606 - acc: 0.9048 - val_loss: 0.4988 - val_acc: 0.8242\n",
      "Epoch 10/20\n",
      "26200/26200 [==============================] - 677s - loss: 0.3393 - acc: 0.9119 - val_loss: 0.4764 - val_acc: 0.8441\n",
      "Epoch 11/20\n",
      "26200/26200 [==============================] - 663s - loss: 0.3092 - acc: 0.9226 - val_loss: 0.4427 - val_acc: 0.8565\n",
      "Epoch 12/20\n",
      "26200/26200 [==============================] - 664s - loss: 0.2920 - acc: 0.9292 - val_loss: 0.4620 - val_acc: 0.8565\n",
      "Epoch 13/20\n",
      "26200/26200 [==============================] - 664s - loss: 0.2814 - acc: 0.9290 - val_loss: 0.4467 - val_acc: 0.8637\n",
      "Epoch 14/20\n",
      "26200/26200 [==============================] - 664s - loss: 0.2602 - acc: 0.9388 - val_loss: 0.4782 - val_acc: 0.8578\n",
      "Epoch 15/20\n",
      "26200/26200 [==============================] - 663s - loss: 0.2769 - acc: 0.9294 - val_loss: 0.4777 - val_acc: 0.8640\n",
      "Epoch 16/20\n",
      "26200/26200 [==============================] - 663s - loss: 0.2327 - acc: 0.9460 - val_loss: 0.4664 - val_acc: 0.8585\n",
      "Epoch 17/20\n",
      "26200/26200 [==============================] - 664s - loss: 0.2372 - acc: 0.9423 - val_loss: 0.5140 - val_acc: 0.8551\n",
      "Epoch 18/20\n",
      "26200/26200 [==============================] - 665s - loss: 0.2191 - acc: 0.9481 - val_loss: 0.5154 - val_acc: 0.8503\n",
      "Epoch 19/20\n",
      "26200/26200 [==============================] - 664s - loss: 0.2083 - acc: 0.9503 - val_loss: 0.5222 - val_acc: 0.8647\n",
      "Epoch 20/20\n",
      "26200/26200 [==============================] - 663s - loss: 0.2060 - acc: 0.9492 - val_loss: 0.4923 - val_acc: 0.8647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa36b53a2b0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#configure the learning process\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "#Start Training\n",
    "model.fit([X_train,features_train], y_train, nb_epoch = 20, batch_size=128, validation_split=0.1)  # we pass one data array per model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.61%\n"
     ]
    }
   ],
   "source": [
    "#Save The Trained Model \n",
    "model.save('functional_model.h5')\n",
    "\n",
    "#Predict Score For the test set for calculating accuracy\n",
    "scores = model.evaluate([X_test,features_test], y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
